# Week 7 (E): Softmax Regression 

The goal of this task is to implement the **softmax regression** model and apply it to 
[Letter Recognition Data Set](https://archive.ics.uci.edu/ml/datasets/Letter+Recognition). The dataset contains 16 numerical features, which are derived from images of capital letters of the English alphabet. The task is to classify the inputs into 26 classes, each corresponding to one of the alphabet letters.
You need to:

- Implement a ```SoftmaxRegression``` class as a child class of ```LinearMulticlassClassification```. The softmax regression model uses the so-called **softmax activation function** given by: 
$$h(x) = {\operatorname{softmax}} (W^Tx + B)\in\mathbb{R}^K \Leftrightarrow [h(x)]_k = \dfrac{e^{w_k^Tx+b_k}}{\sum_{j=1}^K e^{w_j^Tx+b_j}}, 1\leq k \leq K,$$
where $K\in\mathbb{N}$ is the number of classes, $d\in\mathbb{N}$ is the dimensionality (number of features) of $x\in\mathcal{X}$, $W=[w_1,\dots,w_2]\in\mathbb{R}^{d\times K}$ is the matrix of coefficients and $B=[b_1,\dots,b_k]\in\mathbb{R}^{K}$ is the vector of biases. The model also uses **cross-entropy loss** given by:
$$\mathcal{R}(y,\hat{y}) = \sum_{k=1}^K y[k]\log \bigl(\hat{y}[k]\bigr),$$
where we assumed that the outputs are given by one-hot encoding of the class labels, i.e. $y=[0,\dots 1,\dots 0]$ where the index of $'1'$ indicates to the class. You can use full-batch GD for training (already implemented in the exercise class).
- Load the data from the following link: https://archive.ics.uci.edu/ml/machine-learning-databases/letter-recognition/letter-recognition.data
- Shuffle the data and split it into train and test subsets. 
- Train the softmax regression model. Plot the learning curve. Calculate the final train and test accuracy.


**Note**: 
- Softmax regression model is a generalization of the logistic regression model for multi-class problems, which assumes that the data is generated by a **multinomial distribution** instead of a binomial distribution. You can prove this as an exercise. 
- The softmax activation function ensures that the sum of probabilities correspinding to different classes is equal to one.
- Cross-entropy loss used in the softmax regression model is a generalization of the binary cross-entropy loss used in the logistic regression model.




---
Â© [Mariia Seleznova](https://www.ai.math.uni-muenchen.de/members/phd_students/seleznova/index.html) 2022

https://github.com/mselezniova/MSML22

Distributed under the [Creative Commons Attribution License](https://creativecommons.org/licenses/by/4.0/)




